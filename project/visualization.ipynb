{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Importing Libraries and Defining Paths\n",
    "This cell imports essential libraries for the project and sets up configuration paths.\n",
    "\n",
    "### Libraries Imported:\n",
    "- `os`: For interacting with the operating system.\n",
    "- `numpy`: For numerical operations.\n",
    "- `pandas`: For data manipulation.\n",
    "- `rasterio`: For reading and writing geospatial raster data.\n",
    "- `subprocess`: For running subprocesses.\n",
    "\n",
    "### Paths:\n",
    "- Sets paths for training, validation, and testing datasets.\n",
    "- Sets paths for storing outputs and logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import config\n",
    "import pathlib\n",
    "import math\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import *\n",
    "import earthpy.plot as ep\n",
    "import earthpy.spatial as es\n",
    "from dataset import read_img\n",
    "from matplotlib import pyplot as plt\n",
    "import subprocess\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config.train_dir)\n",
    "test_df =  pd.read_csv(config.test_dir)\n",
    "valid_df = pd.read_csv(config.valid_dir)\n",
    "\n",
    "p_train_json = config.p_train_dir\n",
    "p_test_json = config.p_test_dir\n",
    "p_valid_json = config.p_valid_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Counting Images in Datasets\n",
    "This cell prints the total number of images in the training, testing, and validation datasets.\n",
    "\n",
    "### Outputs:\n",
    "- Total number of training images.\n",
    "- Total number of test images.\n",
    "- Total number of validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images = 80\n",
      "Total number of test images = 10\n",
      "Total number of validation images = 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of training images = {len(train_df)}\")\n",
    "print(f\"Total number of test images = {len(test_df)}\")\n",
    "print(f\"Total number of validation images = {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checking Class Balance\n",
    "This cell defines a function to check the class percentage in the full dataset.\n",
    "\n",
    "### Function: `class_balance_check(patchify, data_dir)`\n",
    "- **Parameters**:\n",
    "  - `patchify` (bool): TRUE if class balance is to be checked for patchify experiments.\n",
    "  - `data_dir` (str): Directory where data files are saved.\n",
    "- **Returns**: Class percentage.\n",
    "- **Prints**:\n",
    "  - Class pixel percentage.\n",
    "  - Unique values in the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance_check(patchify, data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        checking class percentage in full dataset\n",
    "    Arguments:\n",
    "        patchify (bool): TRUE if want to check class balance for patchify experiments\n",
    "        data_dir (str): directory where data files are saved \n",
    "    Return:\n",
    "        class percentage\n",
    "    \"\"\"\n",
    "    if patchify:\n",
    "        with open(data_dir, \"r\") as j:\n",
    "            train_data = json.loads(j.read())\n",
    "        labels = train_data[\"masks\"]\n",
    "        patch_idx = train_data[\"patch_idx\"]\n",
    "\n",
    "    else:\n",
    "        train_data = pd.read_csv(data_dir)\n",
    "        labels = train_data.masks.values\n",
    "        patch_idx = None\n",
    "\n",
    "    total = 0\n",
    "    class_name = {}\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        with rasterio.open(labels[i]) as l:\n",
    "            mask = l.read(1)\n",
    "        mask[mask == 2] = 0\n",
    "  \n",
    "        if patchify:\n",
    "            idx = patch_idx[i]\n",
    "            mask = mask[idx[0] : idx[1], idx[2] : idx[3]]\n",
    "\n",
    "        total_pix = mask.shape[0] * mask.shape[1]\n",
    "        total += total_pix\n",
    "\n",
    "        dic = {}\n",
    "        keys = np.unique(mask)\n",
    "        for i in keys:\n",
    "            dic[i] = np.count_nonzero(mask == i)\n",
    "\n",
    "        for key, value in dic.items():\n",
    "            if key in class_name.keys():\n",
    "                class_name[key] = value + class_name[key]\n",
    "            else:\n",
    "                class_name[key] = value\n",
    "\n",
    "    for key, val in class_name.items():\n",
    "        class_name[key] = (val / total) * 100\n",
    "\n",
    "    print(\"Class percentage:\")\n",
    "    for key, val in class_name.items():\n",
    "        print(\"class pixel: {} = {}\".format(key, val))\n",
    "    print(f\"unique value in the mask {class_name.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Class Balance Check\n",
    "This cell runs the `class_balance_check` function on the dataset.\n",
    "\n",
    "### Outputs:\n",
    "- Class percentage for each class in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class percentage of traning data before patch\n",
      "Class percentage:\n",
      "class pixel: 0.0 = 92.04329490661621\n",
      "class pixel: 1.0 = 7.95670509338379\n",
      "unique value in the mask dict_keys([0.0, 1.0])\n",
      ".........................................................................................\n",
      "class percentage of traning data after patch\n",
      "Class percentage:\n",
      "class pixel: 0.0 = 92.06820170084636\n",
      "class pixel: 1.0 = 7.9317982991536455\n",
      "unique value in the mask dict_keys([0.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "print(\"class percentage of traning data before patch\")\n",
    "class_balance_check(patchify=False, data_dir=config.train_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"class percentage of traning data after patch\")\n",
    "class_balance_check(patchify=True, data_dir=config.p_train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checking Unique Height and Width of Images\n",
    "This cell defines a function `check_height_width` to check and print unique heights and widths of images and masks in a dataset.\n",
    "\n",
    "### Function: `check_height_width(data_dir)`\n",
    "- **Parameters**: \n",
    "  - `data_dir` (str): Path to the CSV file.\n",
    "- **Process**:\n",
    "  - Reads the CSV file.\n",
    "  - Extracts image and mask paths.\n",
    "  - Iterates through the images and masks to find unique shapes.\n",
    "  - Prints the shapes of the dataset, input images, and masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_height_width(data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        check unique hight and width of images from dataset\n",
    "    Arguments:\n",
    "        data_dir (str): path to csv file\n",
    "    Return:\n",
    "        print all the unique height and width\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_dir)\n",
    "\n",
    "\n",
    "    print(\"Number of Datasets:  \", data.shape[0])\n",
    "\n",
    "    input_img = data.feature_ids.values\n",
    "    input_mask = data.masks.values\n",
    "\n",
    "    vv_img_shape = []\n",
    "    vh_img_shape = []\n",
    "    dem_img_shape = []\n",
    "    mask_img_shape = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_vv.tif\")) as vv:\n",
    "            vv_shape = vv.shape\n",
    "        if vv_shape not in vv_img_shape:\n",
    "            vv_img_shape.append(vv_shape)\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_vh.tif\")) as vh:\n",
    "            vh_shape = vh.shape\n",
    "        if vh_shape not in vh_img_shape:\n",
    "            vh_img_shape.append(vh_shape)\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_nasadem.tif\")) as dem:\n",
    "            dem_shape = dem.shape\n",
    "        if dem_shape not in dem_img_shape:\n",
    "            dem_img_shape.append(dem_shape)\n",
    "        with rasterio.open((data.masks.values[i])) as mask:\n",
    "            mask_shape = mask.shape\n",
    "        if mask_shape not in mask_img_shape:\n",
    "            mask_img_shape.append(mask_shape)\n",
    "\n",
    "    print(f\"vv_img_shape: {vv_img_shape}\")\n",
    "    print(f\"vh_img_shape: {vh_img_shape}\")\n",
    "    print(f\"dem_img_shape: {dem_img_shape}\")\n",
    "    print(f\"mask_img_shape: {mask_img_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Checking Image Dimensions in Different Datasets\n",
    "This cell prints the unique heights and widths of images and masks for training, testing, and validation datasets by calling the `check_height_width` function.\n",
    "\n",
    "### Actions:\n",
    "- Checks and prints unique image and mask dimensions for the training dataset.\n",
    "- Checks and prints unique image and mask dimensions for the testing dataset.\n",
    "- Checks and prints unique image and mask dimensions for the validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique height and width of training dataset\n",
      "Number of Datasets:   80\n",
      "vv_img_shape: [(512, 512)]\n",
      "vh_img_shape: [(512, 512)]\n",
      "dem_img_shape: [(512, 512)]\n",
      "mask_img_shape: [(512, 512)]\n",
      ".........................................................................................\n",
      "Unique height and width of testing dataset\n",
      "Number of Datasets:   10\n",
      "vv_img_shape: [(512, 512)]\n",
      "vh_img_shape: [(512, 512)]\n",
      "dem_img_shape: [(512, 512)]\n",
      "mask_img_shape: [(512, 512)]\n",
      ".........................................................................................\n",
      "Unique height and width of validation dataset\n",
      "Number of Datasets:   10\n",
      "vv_img_shape: [(512, 512)]\n",
      "vh_img_shape: [(512, 512)]\n",
      "dem_img_shape: [(512, 512)]\n",
      "mask_img_shape: [(512, 512)]\n",
      ".........................................................................................\n",
      "Unique height and width of evaluation dataset\n",
      "Number of Datasets:   100\n",
      "vv_img_shape: [(512, 512)]\n",
      "vh_img_shape: [(512, 512)]\n",
      "dem_img_shape: [(512, 512)]\n",
      "mask_img_shape: [(512, 512)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique height and width of training dataset\")\n",
    "check_height_width(config.train_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"Unique height and width of testing dataset\")\n",
    "check_height_width(config.test_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"Unique height and width of validation dataset\")\n",
    "check_height_width(config.valid_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"Unique height and width of evaluation dataset\")\n",
    "check_height_width(config.eval_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting Metrics from CSV Files\n",
    "This cell defines functions to handle CSV files and plot metrics against epochs.\n",
    "\n",
    "### Functions:\n",
    "- `return_csv_from_path`: Returns a list of CSV file paths from a directory.\n",
    "- `_plot_from_csv`: Plots specified columns from a CSV file against epochs.\n",
    "- `plot_metrics_vs_epochs`: Plots metrics from a CSV file against epochs using `_plot_from_csv`.\n",
    "- `plot_metric_vs_epochs_vs_models`: Plots a specific metric against epochs for different models and saves the combined results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_csv_from_path(csv_path=config.csv_logger_path/ \"fapnet\"):\n",
    "    csv_list = []\n",
    "    # Iterate through each subdirectory\n",
    "    for folder in csv_path.iterdir():\n",
    "        # Check if the entry is a directory\n",
    "        if folder.is_dir():\n",
    "            # Iterate through files in the subdirectory\n",
    "            for file in folder.iterdir():\n",
    "                # Check if the entry is a file\n",
    "                if file.is_file():\n",
    "                    csv_list.append(file)\n",
    "    # print(csv_list)\n",
    "    return csv_list\n",
    "                    \n",
    "\n",
    "def _plot_from_csv(csv_path, name, x_axis_name, y_axis_name, columns_to_plot=None, upto_epoch=None):\n",
    "    pathlib.Path((config.root_dir /\"logs\" / \"plots\"/\"metrics_plots\")).mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if upto_epoch is not None:\n",
    "        df = df.head(upto_epoch)\n",
    "        print(df.shape)\n",
    "    epochs = df['epoch']\n",
    "    if columns_to_plot is not None:\n",
    "        columns_to_plot = columns_to_plot\n",
    "    else:\n",
    "        columns_to_plot = df.columns.to_list()[1:]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for column in columns_to_plot:\n",
    "        plt.plot(epochs, df[column], label=column, linewidth=3.0,\n",
    "            marker=\"o\",\n",
    "            markersize=5)\n",
    "\n",
    "    plt.title(f\"{y_axis_name}_over_{x_axis_name}\")\n",
    "    plt.xlabel(x_axis_name)\n",
    "    plt.ylabel(y_axis_name)\n",
    "    plt.xticks(epochs.astype(int))\n",
    "    plt.legend()\n",
    "    plt.savefig(config.root_dir/\"logs\"/\"plots\"/\"metrics_plots\"/name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics_vs_epochs(csv_path, name, x_axis_name= \"Epochs\", y_axis_name=\"Metrics_score\",columns_to_plot=None, upto_epoch=None):\n",
    "    _plot_from_csv(csv_path=csv_path, name=name,x_axis_name=x_axis_name, y_axis_name=y_axis_name, columns_to_plot=columns_to_plot, upto_epoch=upto_epoch)\n",
    "\n",
    "def plot_metric_vs_epochs_vs_models(metric_name=\"val_f1-score\"):\n",
    "    pathlib.Path((config.root_dir /\"logs\"/ \"plots\"/\"csv_for_plotting\")).mkdir(parents=True, exist_ok=True)\n",
    "    csv_list = return_csv_from_path()\n",
    "    result_df = pd.DataFrame()\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        result_df[os.path.basename(csv_path)] = df[metric_name]\n",
    "    result_df.index.name = \"epoch\"\n",
    "    result_df.to_csv(os.path.join(config.root_dir/\"logs\"/\"plots\"/\"csv_for_plotting\"/f\"{metric_name}_vs_epoch.csv\"), encoding='utf-8',index=True, header=True)\n",
    "    _plot_from_csv(config.root_dir/\"logs\"/\"plots\"/\"csv_for_plotting\"/f\"{metric_name}_vs_epoch.csv\", x_axis_name= \"Epochs\", y_axis_name=metric_name, name=metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 11)\n"
     ]
    }
   ],
   "source": [
    "plot_metrics_vs_epochs(\"/mnt/hdd2/mdsamiul/project/imseg_sar_csml/logs/ahmed/unet/unet_ex_Band123_ep_1000_11-Nov-23.csv\",'123',columns_to_plot=[\"f1_score\",\"val_f1_score\"],  upto_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 11)\n"
     ]
    }
   ],
   "source": [
    "plot_metrics_vs_epochs(\"/mnt/hdd2/mdsamiul/project/imseg_sar_csml/logs/ahmed/unet/unet_ex_Band123_ep_1000_11-Nov-23.csv\",'123',  upto_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plotting Specific Metrics from CSV Files\n",
    "This cell plots metrics against epochs using previously defined functions.\n",
    "\n",
    "### Actions:\n",
    "- Plots metrics from a specified CSV file.\n",
    "- Plots F1 score from a specified CSV file.\n",
    "- Plots metrics for different models.\n",
    "- Plots recall metric for different models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "plot_metrics_vs_epochs(config.csv_logger_path/\"fapnet\"/\"fapnet_ex_2024-03-20_e_30_p_256_s_128_ep_30.csv\",name='metrics')\n",
    "plot_metrics_vs_epochs(config.csv_logger_path/\"fapnet\"/\"fapnet_ex_2024-03-20_e_30_p_256_s_128_ep_30.csv\",name='metrics',columns_to_plot=[\"f1-score\"])\n",
    "plot_metric_vs_epochs_vs_models()\n",
    "plot_metric_vs_epochs_vs_models(metric_name=\"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Displaying and Saving All Images and Masks\n",
    "Defines `display_all` to save images and their corresponding masks into a single figure for visualization.\n",
    "\n",
    "### Function: `display_all(data, name)`\n",
    "- **Parameters**:\n",
    "  - `data`: Data file holding image paths.\n",
    "  - `name` (str): Path to save images.\n",
    "- **Process**:\n",
    "  - Reads and processes each image and mask.\n",
    "  - Displays images and masks in a figure.\n",
    "  - Saves the figure to the specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(data,name):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        save all images into single figure\n",
    "    Arguments:\n",
    "        data : data file holding images path\n",
    "        directory (str) : path to save images\n",
    "    Return:\n",
    "        save images figure into directory\n",
    "    \"\"\"\n",
    "    \n",
    "    pathlib.Path((visualization_dir / \"display\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"train\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"test\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"valid\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_vv.tif\")) as vv:\n",
    "            vv_img = vv.read(1)\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_vh.tif\")) as vh:\n",
    "            vh_img = vh.read(1)\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_nasadem.tif\")) as dem:\n",
    "            dem_img = dem.read(1)\n",
    "        with rasterio.open((data.masks.values[i])) as l:\n",
    "            lp_img = l.read(1)\n",
    "            lp_img[lp_img==2]=0\n",
    "        id = data.feature_ids.values[i].split(\"/\")[-1]\n",
    "        display_list = {\n",
    "                     \"vv\":vv_img,\n",
    "                     \"vh\":vh_img,\n",
    "                     \"dem\":dem_img,\n",
    "                     \"label\":lp_img}\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        title = list(display_list.keys())\n",
    "\n",
    "        for i in range(len(display_list)):\n",
    "            plt.subplot(1, len(display_list), i+1)\n",
    "            \n",
    "            # plot dem channel using earthpy\n",
    "            if title[i]==\"dem\":\n",
    "                ax = plt.gca()\n",
    "                hillshade = es.hillshade(display_list[title[i]], azimuth=180)\n",
    "                ep.plot_bands(\n",
    "                    display_list[title[i]],\n",
    "                    cbar=False,\n",
    "                    cmap=\"terrain\",\n",
    "                    title=title[i],\n",
    "                    ax=ax\n",
    "                )\n",
    "                ax.imshow(hillshade, cmap=\"Greys\", alpha=0.5)\n",
    "            \n",
    "            # gray image plot vv and vh channels\n",
    "            elif title[i]==\"vv\" or title[i]==\"vh\":\n",
    "                plt.title(title[i])\n",
    "                plt.imshow((display_list[title[i]]), cmap=\"gray\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "            # gray label plot\n",
    "            elif title[i]==\"label\":\n",
    "                plt.title(title[i])\n",
    "                plt.imshow((display_list[title[i]]), cmap=\"gray\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "            # rgb plot\n",
    "            else:\n",
    "                plt.title(title[i])\n",
    "                plt.imshow((display_list[title[i]]))\n",
    "                plt.axis('off')\n",
    "\n",
    "        prediction_name = \"img_id_{}.png\".format(id) # create file name to save\n",
    "        plt.savefig(os.path.join((config.visualization_dir / 'display'/ name), prediction_name), bbox_inches='tight', dpi=800)\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Displaying Training Images and Masks\n",
    "Displays and saves training images and masks using the `display_all` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying training images and masks\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying training images and masks\")\n",
    "display_all(data=train_df,name=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Displaying Testing Images and Masks\n",
    "Displays and saves testing images and masks using the `display_all` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying testing images and masks\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying testing images and masks\")\n",
    "display_all(data=test_df,name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Displaying Validation Images and Masks\n",
    "Displays and saves validation images and masks using the `display_all` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying validation images and masks\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying validation images and masks\")\n",
    "display_all(data=valid_df,name=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/hdd2/mdsamiul/project/imseg_sar_csml/data/dataset/tile_5_5_2018_2019_w5000_h5000_id_4_vv.tif', '/mnt/hdd2/mdsamiul/project/imseg_sar_csml/data/dataset/tile_5_5_2018_2019_w5000_h5000_id_4_vh.tif', '/mnt/hdd2/mdsamiul/project/imseg_sar_csml/data/dataset/tile_5_5_2018_2019_w5000_h5000_id_4_nasadem.tif']\n",
      "...............................\n",
      "-8.962684746929048\n",
      "5.619849059325676\n",
      "...............................\n",
      "...............................\n",
      "-15.133013429673001\n",
      "5.499652248889546\n",
      "...............................\n",
      "...............................\n",
      "754.1671180725098\n",
      "271.5426510050796\n",
      "...............................\n"
     ]
    }
   ],
   "source": [
    "eval_csv = pd.read_csv(\"/mnt/hdd2/mdsamiul/project/imseg_sar_csml/data/csv/train.csv\")\n",
    "masks = eval_csv[\"feature_ids\"].to_list()\n",
    "ext = [\"_vv.tif\",\"_vh.tif\",\"_nasadem.tif\"]\n",
    "masks = masks[0]\n",
    "masks= [masks+ex for ex in ext]\n",
    "print(masks)\n",
    "for p in masks:\n",
    "    with rasterio.open(p) as im:\n",
    "        image = im.read(1)\n",
    "    print(\"...............................\")\n",
    "    print(np.mean(image))\n",
    "    print(np.std(image))\n",
    "    print(\"...............................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Saving Image Tiles\n",
    "Defines `save_tiles` to split large images into smaller tiles and save them.\n",
    "\n",
    "### Function: `save_tiles(path, out_path, tiles_size=512, stride=512)`\n",
    "- **Parameters**:\n",
    "  - `path`: Directory with original images.\n",
    "  - `out_path`: Directory to save the tiles.\n",
    "  - `tiles_size`: Size of each tile.\n",
    "  - `stride`: Stride for tiling.\n",
    "- **Process**: Iterates through images, splits them into tiles, and saves the tiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tiles(path, out_path, tiles_size=512, stride=512):\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each file in the path\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Get metadata and calculate number of tiles in each dimension\n",
    "            meta = src.meta\n",
    "            meta[\"height\"]= tiles_size\n",
    "            meta[\"width\"]= tiles_size\n",
    "            # print(meta)\n",
    "            height, width = src.shape\n",
    "            num_rows = math.ceil((height - tiles_size) / stride + 1)\n",
    "            num_cols = math.ceil((width - tiles_size) / stride + 1)\n",
    "            total_tiles = num_rows* num_cols\n",
    "            print(f\"shape of the image before tiles : {src.shape}\")\n",
    "            print(f\"number of tiles={total_tiles}\")\n",
    "            print(\"..................................................\")\n",
    "            # Iterate over each tile\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_cols):\n",
    "                    # Calculate window coordinates\n",
    "                    row_start = row * stride\n",
    "                    row_stop = min(row_start + tiles_size, height)\n",
    "                    col_start = col * stride\n",
    "                    col_stop = min(col_start + tiles_size, width)\n",
    "                    \n",
    "                    # Read the tile data\n",
    "                    window = Window.from_slices((row_stop-stride, row_stop), (col_stop-stride, col_stop))\n",
    "                    tile_data = src.read(window=window)\n",
    "                    # print(\"...........\")\n",
    "                    # print(tile_data.shape)\n",
    "                    # Save the tile with a suffix of tile id\n",
    "                    out_filename = f\"tile_{row}_{col}_{os.path.splitext(filename)[0]}.tif\"\n",
    "                    out_file_path = os.path.join(out_path, out_filename)\n",
    "                    with rasterio.open(out_file_path, 'w', **meta) as dst:\n",
    "                        dst.write(tile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil((5000 - 512) / 256 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(\"/mnt/hdd2/mdsamiul/project/dataset/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_tiles(dataset_dir,config.root_dir/\"tiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/hdd2/mdsamiul/project/imseg_sar_csml/data/dataset')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(datapath):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(datapath)\n",
    "    \n",
    "    for filename in files:\n",
    "        # Extract the file extension\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        \n",
    "        # Check if the filename starts with DEM_ab.tif\n",
    "        if filename.startswith(\"DEM_\"):\n",
    "            new_filename = filename.replace(\"DEM_\", \"\").replace(\".tif\", \"_nasadem.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VV_ab.tif\n",
    "        elif filename.startswith(\"VV_\"):\n",
    "            new_filename = filename.replace(\"VV_\", \"\").replace(\".tif\", \"_vv.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VH_ab.tif\n",
    "        elif filename.startswith(\"VH_\"):\n",
    "            new_filename = filename.replace(\"VH_\", \"\").replace(\".tif\", \"_vh.tif\")\n",
    "        \n",
    "        # Check if the filename starts with GT_ab.tif\n",
    "        elif filename.startswith(\"GT_\"):\n",
    "            new_filename = filename.replace(\"GT_\", \"\")\n",
    "        \n",
    "        else:\n",
    "            # If none of the conditions are met, skip this file\n",
    "            raise ValueError(\"files_name_mismatch\")\n",
    "        \n",
    "        # Construct the new filepath\n",
    "        new_filepath = os.path.join(datapath, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(os.path.join(datapath, filename), new_filepath)\n",
    "        print(f\"Renamed {filename} to {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "files_name_mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3426217/3134201167.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrename_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3426217/2230644808.py\u001b[0m in \u001b[0;36mrename_files\u001b[0;34m(datapath)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# If none of the conditions are met, skip this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files_name_mismatch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Construct the new filepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: files_name_mismatch"
     ]
    }
   ],
   "source": [
    "datapath = config.dataset_dir\n",
    "rename_files(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal output saved to data_statistics.rtf\n"
     ]
    }
   ],
   "source": [
    "# Run the command in the terminal\n",
    "command = \"python visualization.py\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Get the terminal output\n",
    "terminal_output = result.stdout\n",
    "\n",
    "# Save the output to an RTF file\n",
    "rtf_filename = \"data_statistics.rtf\"\n",
    "with open(rtf_filename, \"w\") as rtf_file:\n",
    "    # rtf_file.write(\"{\\\\rtf1\\\\ansi\\n\")\n",
    "    rtf_file.write(terminal_output)\n",
    "    # rtf_file.write(\"}\")\n",
    "\n",
    "print(f\"Terminal output saved to {rtf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
