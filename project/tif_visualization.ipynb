{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import config\n",
    "import pathlib\n",
    "import math\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import *\n",
    "import earthpy.plot as ep\n",
    "import earthpy.spatial as es\n",
    "from dataset import read_img\n",
    "from matplotlib import pyplot as plt\n",
    "import subprocess\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config.train_dir)\n",
    "test_df =  pd.read_csv(config.test_dir)\n",
    "valid_df = pd.read_csv(config.valid_dir)\n",
    "\n",
    "p_train_json = config.p_train_dir\n",
    "p_test_json = config.p_test_dir\n",
    "p_valid_json = config.p_valid_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images = 80\n",
      "Total number of test images = 10\n",
      "Total number of validation images = 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of training images = {len(train_df)}\")\n",
    "print(f\"Total number of test images = {len(test_df)}\")\n",
    "print(f\"Total number of validation images = {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance_check(patchify, data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        checking class percentage in full dataset\n",
    "    Arguments:\n",
    "        patchify (bool): TRUE if want to check class balance for patchify experiments\n",
    "        data_dir (str): directory where data files are saved \n",
    "    Return:\n",
    "        class percentage\n",
    "    \"\"\"\n",
    "    if patchify:\n",
    "        with open(data_dir, \"r\") as j:\n",
    "            train_data = json.loads(j.read())\n",
    "        labels = train_data[\"masks\"]\n",
    "        patch_idx = train_data[\"patch_idx\"]\n",
    "\n",
    "    # commented out by manik (as patchify is false, it's CFR or CFR_CB, so it's deprecated)\n",
    "    else:\n",
    "        train_data = pd.read_csv(data_dir)\n",
    "        labels = train_data.masks.values\n",
    "        patch_idx = None\n",
    "\n",
    "    total = 0\n",
    "    class_name = {}\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        with rasterio.open(labels[i]) as l:\n",
    "            mask = l.read(1)\n",
    "        mask[mask == 2] = 0\n",
    "        # mask[mask < 105] = 0\n",
    "        # mask[mask > 104] = 1\n",
    "        if patchify:\n",
    "            idx = patch_idx[i]\n",
    "            mask = mask[idx[0] : idx[1], idx[2] : idx[3]]\n",
    "\n",
    "        total_pix = mask.shape[0] * mask.shape[1]\n",
    "        total += total_pix\n",
    "\n",
    "        dic = {}\n",
    "        keys = np.unique(mask)\n",
    "        for i in keys:\n",
    "            dic[i] = np.count_nonzero(mask == i)\n",
    "\n",
    "        for key, value in dic.items():\n",
    "            if key in class_name.keys():\n",
    "                #problems\n",
    "                class_name[key] = value + class_name[key]\n",
    "            else:\n",
    "                class_name[key] = value\n",
    "\n",
    "    for key, val in class_name.items():\n",
    "        class_name[key] = (val / total) * 100\n",
    "\n",
    "    print(\"Class percentage:\")\n",
    "    for key, val in class_name.items():\n",
    "        print(\"class pixel: {} = {}\".format(key, val))\n",
    "    print(f\"unique value in the mask {class_name.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class percentage of traning data before patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class percentage:\n",
      "class pixel: 0.0 = 92.04329490661621\n",
      "class pixel: 1.0 = 7.95670509338379\n",
      "unique value in the mask dict_keys([0.0, 1.0])\n",
      ".........................................................................................\n",
      "class percentage of traning data after patch\n",
      "Class percentage:\n",
      "class pixel: 0.0 = 92.06820170084636\n",
      "class pixel: 1.0 = 7.9317982991536455\n",
      "unique value in the mask dict_keys([0.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "print(\"class percentage of traning data before patch\")\n",
    "class_balance_check(patchify=False, data_dir=config.train_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"class percentage of traning data after patch\")\n",
    "class_balance_check(patchify=True, data_dir=config.p_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_height_width(data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        check unique hight and width of images from dataset\n",
    "    Arguments:\n",
    "        data_dir (str): path to csv file\n",
    "    Return:\n",
    "        print all the unique height and width\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_dir)\n",
    "    # removing UU or UMM or UM\n",
    "    # data = data[data['feature_ids'].str.contains('uu_00') == False]\n",
    "    #problems\n",
    "    # data = data[data[\"feature_ids\"].str.contains(\"umm_00\") == False]\n",
    "    # data = data[data[\"feature_ids\"].str.contains(\"um_00\") == False]\n",
    "\n",
    "    print(\"Number of Datasets:  \", data.shape[0])\n",
    "\n",
    "    input_img = data.feature_ids.values\n",
    "    input_mask = data.masks.values\n",
    "\n",
    "    vv_img_shape = []\n",
    "    vh_img_shape = []\n",
    "    dem_img_shape = []\n",
    "    mask_img_shape = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_vv.tif\")) as vv:\n",
    "            vv_shape = vv.shape\n",
    "        if vv_shape not in vv_img_shape:\n",
    "            vv_img_shape.append(vv_shape)\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_vh.tif\")) as vh:\n",
    "            vh_shape = vh.shape\n",
    "        if vh_shape not in vh_img_shape:\n",
    "            vh_img_shape.append(vh_shape)\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_nasadem.tif\")) as dem:\n",
    "            dem_shape = dem.shape\n",
    "        if dem_shape not in dem_img_shape:\n",
    "            dem_img_shape.append(dem_shape)\n",
    "        with rasterio.open((data.masks.values[i])) as mask:\n",
    "            mask_shape = mask.shape\n",
    "        if mask_shape not in mask_img_shape:\n",
    "            # print(mask)\n",
    "            mask_img_shape.append(mask_shape)\n",
    "\n",
    "    print(f\"vv_img_shape: {vv_img_shape}\")\n",
    "    print(f\"vh_img_shape: {vh_img_shape}\")\n",
    "    print(f\"dem_img_shape: {dem_img_shape}\")\n",
    "    print(f\"mask_img_shape: {mask_img_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique height and width of training dataset\n",
      "Number of Datasets:   80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vv_img_shape: [(512, 512)]\n",
      "vh_img_shape: [(512, 512)]\n",
      "dem_img_shape: [(512, 512)]\n",
      "mask_img_shape: [(512, 512)]\n",
      ".........................................................................................\n",
      "Unique height and width of testing dataset\n",
      "Number of Datasets:   10\n",
      "vv_img_shape: [(512, 512)]\n",
      "vh_img_shape: [(512, 512)]\n",
      "dem_img_shape: [(512, 512)]\n",
      "mask_img_shape: [(512, 512)]\n",
      ".........................................................................................\n",
      "Unique height and width of validation dataset\n",
      "Number of Datasets:   10\n",
      "vv_img_shape: [(512, 512)]\n",
      "vh_img_shape: [(512, 512)]\n",
      "dem_img_shape: [(512, 512)]\n",
      "mask_img_shape: [(512, 512)]\n",
      ".........................................................................................\n",
      "Unique height and width of evaluation dataset\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/hdd2/mdsamiul/project/imseg_sar_csml/data/csv/eval.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1033102/2107044794.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".........................................................................................\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unique height and width of evaluation dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcheck_height_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1033102/2174355097.py\u001b[0m in \u001b[0;36mcheck_height_width\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# removing UU or UMM or UM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# data = data[data['feature_ids'].str.contains('uu_00') == False]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/hdd2/mdsamiul/project/imseg_sar_csml/data/csv/eval.csv'"
     ]
    }
   ],
   "source": [
    "print(\"Unique height and width of training dataset\")\n",
    "check_height_width(config.train_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"Unique height and width of testing dataset\")\n",
    "check_height_width(config.test_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"Unique height and width of validation dataset\")\n",
    "check_height_width(config.valid_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"Unique height and width of evaluation dataset\")\n",
    "check_height_width(config.eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datasets:   21\n",
      "<closed DatasetReader name='/mnt/hdd2/mdsamiul/project/dataset/rice_data_training/awc00.tif' mode='r'>\n",
      "<closed DatasetReader name='/mnt/hdd2/mdsamiul/project/dataset/rice_data_training/2018_2019_w5000_h5000_id_4.tif' mode='r'>\n",
      "vv_img_shape: [(1614, 2101), (5000, 5000)]\n",
      "vh_img_shape: [(1614, 2101), (5000, 5000)]\n",
      "dem_img_shape: [(1614, 2101), (5000, 5000)]\n",
      "mask_img_shape: [(1490, 1939), (5000, 5000)]\n"
     ]
    }
   ],
   "source": [
    "check_height_width(\"/mnt/hdd2/mdsamiul/project/imseg_csml/data/csv/all_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_csv_from_path(csv_path=config.csv_logger_path/ \"logs/ahmed/unet\"):\n",
    "    csv_list = []\n",
    "    # Iterate through each subdirectory\n",
    "    for folder in csv_path.iterdir():\n",
    "        # Check if the entry is a directory\n",
    "        if folder.is_dir():\n",
    "            # Iterate through files in the subdirectory\n",
    "            for file in folder.iterdir():\n",
    "                # Check if the entry is a file\n",
    "                if file.is_file():\n",
    "                    csv_list.append(file)\n",
    "    # print(csv_list)\n",
    "    return csv_list\n",
    "                    \n",
    "\n",
    "def _plot_from_csv(csv_path, name, x_axis_name, y_axis_name, columns_to_plot=None, upto_epoch=None):\n",
    "    pathlib.Path((config.root_dir /\"logs\" / \"plots\"/\"metrics_plots\")).mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if upto_epoch is not None:\n",
    "        df = df.head(upto_epoch)\n",
    "        print(df.shape)\n",
    "    epochs = df['epoch']\n",
    "    if columns_to_plot is not None:\n",
    "        columns_to_plot = columns_to_plot\n",
    "    else:\n",
    "        columns_to_plot = df.columns.to_list()[1:]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for column in columns_to_plot:\n",
    "        plt.plot(epochs, df[column], label=column, linewidth=3.0,\n",
    "            marker=\"o\",\n",
    "            markersize=5)\n",
    "\n",
    "    plt.title(f\"{y_axis_name}_over_{x_axis_name}\")\n",
    "    plt.xlabel(x_axis_name)\n",
    "    plt.ylabel(y_axis_name)\n",
    "    plt.xticks(epochs.astype(int))\n",
    "    plt.legend()\n",
    "    plt.savefig(config.root_dir/\"logs\"/\"plots\"/\"metrics_plots\"/name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics_vs_epochs(csv_path, name, x_axis_name= \"Epochs\", y_axis_name=\"Metrics_score\",columns_to_plot=None, upto_epoch=None):\n",
    "    _plot_from_csv(csv_path=csv_path, name=name,x_axis_name=x_axis_name, y_axis_name=y_axis_name, columns_to_plot=columns_to_plot, upto_epoch=upto_epoch)\n",
    "\n",
    "def plot_metric_vs_epochs_vs_models(metric_name=\"val_f1-score\"):\n",
    "    pathlib.Path((config.root_dir /\"logs\"/ \"plots\"/\"csv_for_plotting\")).mkdir(parents=True, exist_ok=True)\n",
    "    csv_list = return_csv_from_path()\n",
    "    result_df = pd.DataFrame()\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        result_df[os.path.basename(csv_path)] = df[metric_name]\n",
    "    result_df.index.name = \"epoch\"\n",
    "    result_df.to_csv(os.path.join(config.root_dir/\"logs\"/\"plots\"/\"csv_for_plotting\"/f\"{metric_name}_vs_epoch.csv\"), encoding='utf-8',index=True, header=True)\n",
    "    _plot_from_csv(config.root_dir/\"logs\"/\"plots\"/\"csv_for_plotting\"/f\"{metric_name}_vs_epoch.csv\", x_axis_name= \"Epochs\", y_axis_name=metric_name, name=metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 11)\n"
     ]
    }
   ],
   "source": [
    "plot_metrics_vs_epochs(\"/mnt/hdd2/mdsamiul/project/imseg_csml/logs/ahmed/unet/unet_ex_Band123_ep_1000_11-Nov-23.csv\",'123',columns_to_plot=[\"f1_score\",\"val_f1_score\"],  upto_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 11)\n"
     ]
    }
   ],
   "source": [
    "plot_metrics_vs_epochs(\"/mnt/hdd2/mdsamiul/project/imseg_csml/logs/ahmed/unet/unet_ex_Band123_ep_1000_11-Nov-23.csv\",'123',  upto_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/hdd2/mdsamiul/project/imseg_csml/logs/csv_logger/logs/ahmed/unet')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.csv_logger_path/ \"logs/ahmed/unet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/hdd2/mdsamiul/project/imseg_csml/logs/csv_logger/fapnet')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.csv_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_vs_epochs(config.csv_logger_path/\"fapnet\"/\"fapnet_ex_2024-03-13_e_100_p_256_s_128_ep_100.csv\",name='metrics')\n",
    "plot_metrics_vs_epochs(config.csv_logger_path/\"fapnet\"/\"fapnet_ex_2024-03-13_e_100_p_256_s_128_ep_100.csv\",name='metrics',columns_to_plot=[\"f1-score\"])\n",
    "plot_metric_vs_epochs_vs_models()\n",
    "plot_metric_vs_epochs_vs_models(metric_name=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(data,name):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        save all images into single figure\n",
    "    Arguments:\n",
    "        data : data file holding images path\n",
    "        directory (str) : path to save images\n",
    "    Return:\n",
    "        save images figure into directory\n",
    "    \"\"\"\n",
    "    \n",
    "    pathlib.Path((visualization_dir / \"display\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"train\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"test\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"valid\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_vv.tif\")) as vv:\n",
    "            vv_img = vv.read(1)\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_vh.tif\")) as vh:\n",
    "            vh_img = vh.read(1)\n",
    "        with rasterio.open((data.feature_ids.values[i]+\"_nasadem.tif\")) as dem:\n",
    "            dem_img = dem.read(1)\n",
    "        with rasterio.open((data.masks.values[i])) as l:\n",
    "            lp_img = l.read(1)\n",
    "            lp_img[lp_img==2]=0\n",
    "        id = data.feature_ids.values[i].split(\"/\")[-1]\n",
    "        display_list = {\n",
    "                     \"vv\":vv_img,\n",
    "                     \"vh\":vh_img,\n",
    "                     \"dem\":dem_img,\n",
    "                     \"label\":lp_img}\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        title = list(display_list.keys())\n",
    "\n",
    "        for i in range(len(display_list)):\n",
    "            plt.subplot(1, len(display_list), i+1)\n",
    "            \n",
    "            # plot dem channel using earthpy\n",
    "            if title[i]==\"dem\":\n",
    "                ax = plt.gca()\n",
    "                hillshade = es.hillshade(display_list[title[i]], azimuth=180)\n",
    "                ep.plot_bands(\n",
    "                    display_list[title[i]],\n",
    "                    cbar=False,\n",
    "                    cmap=\"terrain\",\n",
    "                    title=title[i],\n",
    "                    ax=ax\n",
    "                )\n",
    "                ax.imshow(hillshade, cmap=\"Greys\", alpha=0.5)\n",
    "            \n",
    "            # gray image plot vv and vh channels\n",
    "            elif title[i]==\"vv\" or title[i]==\"vh\":\n",
    "                plt.title(title[i])\n",
    "                plt.imshow((display_list[title[i]]), cmap=\"gray\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "            # gray label plot\n",
    "            elif title[i]==\"label\":\n",
    "                plt.title(title[i])\n",
    "                plt.imshow((display_list[title[i]]), cmap=\"gray\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "            # rgb plot\n",
    "            else:\n",
    "                plt.title(title[i])\n",
    "                plt.imshow((display_list[title[i]]))\n",
    "                plt.axis('off')\n",
    "\n",
    "        prediction_name = \"img_id_{}.png\".format(id) # create file name to save\n",
    "        plt.savefig(os.path.join((config.visualization_dir / 'display'/ name), prediction_name), bbox_inches='tight', dpi=800)\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying training images and masks\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying training images and masks\")\n",
    "display_all(data=train_df,name=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying testing images and masks\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying testing images and masks\")\n",
    "display_all(data=test_df,name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying validation images and masks\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying validation images and masks\")\n",
    "display_all(data=valid_df,name=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/hdd2/mdsamiul/project/dataset/rice_data_training/2018_2019_w5000_h5000_id_4_vv.tif', '/mnt/hdd2/mdsamiul/project/dataset/rice_data_training/2018_2019_w5000_h5000_id_4_vh.tif', '/mnt/hdd2/mdsamiul/project/dataset/rice_data_training/2018_2019_w5000_h5000_id_4_nasadem.tif']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................\n",
      "-12.204613877277495\n",
      "4.0422273221629785\n",
      "...............................\n",
      "...............................\n",
      "-18.96970667660666\n",
      "4.367189151420688\n",
      "...............................\n",
      "...............................\n",
      "812.64422736\n",
      "425.41858324129265\n",
      "...............................\n"
     ]
    }
   ],
   "source": [
    "eval_csv = pd.read_csv(\"/mnt/hdd2/mdsamiul/project/imseg_csml/data/csv/train.csv\")\n",
    "masks = eval_csv[\"feature_ids\"].to_list()\n",
    "ext = [\"_vv.tif\",\"_vh.tif\",\"_nasadem.tif\"]\n",
    "masks = masks[0]\n",
    "masks= [masks+ex for ex in ext]\n",
    "print(masks)\n",
    "for p in masks:\n",
    "    with rasterio.open(p) as im:\n",
    "        image = im.read(1)\n",
    "    print(\"...............................\")\n",
    "    # print(p)\n",
    "    # print(np.unique(image, return_counts=False))\n",
    "    print(np.mean(image))\n",
    "    print(np.std(image))\n",
    "    print(\"...............................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tiles(path, out_path, tiles_size=512, stride=512):\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each file in the path\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Get metadata and calculate number of tiles in each dimension\n",
    "            meta = src.meta\n",
    "            meta[\"height\"]= tiles_size\n",
    "            meta[\"width\"]= tiles_size\n",
    "            # print(meta)\n",
    "            height, width = src.shape\n",
    "            num_rows = math.ceil((height - tiles_size) / stride + 1)\n",
    "            num_cols = math.ceil((width - tiles_size) / stride + 1)\n",
    "            total_tiles = num_rows* num_cols\n",
    "            print(f\"shape of the image before tiles : {src.shape}\")\n",
    "            print(f\"number of tiles={total_tiles}\")\n",
    "            print(\"..................................................\")\n",
    "            # Iterate over each tile\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_cols):\n",
    "                    # Calculate window coordinates\n",
    "                    row_start = row * stride\n",
    "                    row_stop = min(row_start + tiles_size, height)\n",
    "                    col_start = col * stride\n",
    "                    col_stop = min(col_start + tiles_size, width)\n",
    "                    \n",
    "                    # Read the tile data\n",
    "                    # window = Window(x0, y0, x1 - x0, y1 - y0)\n",
    "                    window = Window.from_slices((row_stop-stride, row_stop), (col_stop-stride, col_stop))\n",
    "                    tile_data = src.read(window=window)\n",
    "                    # print(\"...........\")\n",
    "                    # print(tile_data.shape)\n",
    "                    # Save the tile with a suffix of tile id\n",
    "                    # out_filename = f\"{os.path.splitext(filename)[0]}_tile_{row}_{col}.tif\"\n",
    "                    out_filename = f\"tile_{row}_{col}_{os.path.splitext(filename)[0]}.tif\"\n",
    "                    out_file_path = os.path.join(out_path, out_filename)\n",
    "                    with rasterio.open(out_file_path, 'w', **meta) as dst:\n",
    "                        dst.write(tile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil((5000 - 512) / 256 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(\"/mnt/hdd2/mdsamiul/project/dataset/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the image before tiles : (5000, 5000)\n",
      "number of tiles=100\n",
      "..................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the image before tiles : (5000, 5000)\n",
      "number of tiles=100\n",
      "..................................................\n",
      "shape of the image before tiles : (5000, 5000)\n",
      "number of tiles=100\n",
      "..................................................\n",
      "shape of the image before tiles : (5000, 5000)\n",
      "number of tiles=100\n",
      "..................................................\n"
     ]
    }
   ],
   "source": [
    "save_tiles(dataset_dir,config.root_dir/\"tiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/hdd2/mdsamiul/project/dataset/rice_data_training')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(datapath):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(datapath)\n",
    "    \n",
    "    for filename in files:\n",
    "        # Extract the file extension\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        \n",
    "        # Check if the filename starts with DEM_ab.tif\n",
    "        if filename.startswith(\"DEM_\"):\n",
    "            new_filename = filename.replace(\"DEM_\", \"\").replace(\".tif\", \"_nasadem.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VV_ab.tif\n",
    "        elif filename.startswith(\"VV_\"):\n",
    "            new_filename = filename.replace(\"VV_\", \"\").replace(\".tif\", \"_vv.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VH_ab.tif\n",
    "        elif filename.startswith(\"VH_\"):\n",
    "            new_filename = filename.replace(\"VH_\", \"\").replace(\".tif\", \"_vh.tif\")\n",
    "        \n",
    "        # Check if the filename starts with GT_ab.tif\n",
    "        elif filename.startswith(\"GT_\"):\n",
    "            new_filename = filename.replace(\"GT_\", \"\")\n",
    "        \n",
    "        else:\n",
    "            # If none of the conditions are met, skip this file\n",
    "            raise ValueError(\"files_name_mismatch\")\n",
    "        \n",
    "        # Construct the new filepath\n",
    "        new_filepath = os.path.join(datapath, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(os.path.join(datapath, filename), new_filepath)\n",
    "        print(f\"Renamed {filename} to {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = config.dataset_dir\n",
    "rename_files(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal output saved to data_statistics.rtf\n"
     ]
    }
   ],
   "source": [
    "# Run the command in the terminal\n",
    "command = \"python visualization.py\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Get the terminal output\n",
    "terminal_output = result.stdout\n",
    "\n",
    "# Save the output to an RTF file\n",
    "rtf_filename = \"data_statistics.rtf\"\n",
    "with open(rtf_filename, \"w\") as rtf_file:\n",
    "    # rtf_file.write(\"{\\\\rtf1\\\\ansi\\n\")\n",
    "    rtf_file.write(terminal_output)\n",
    "    # rtf_file.write(\"}\")\n",
    "\n",
    "print(f\"Terminal output saved to {rtf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
